{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "import datetime\n",
    "import tweepy\n",
    "import json \n",
    "import csv\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for writing to metadata \n",
    "# input the metadatfile to write to; input the row to write \n",
    "\n",
    "def metadata_writer(metadata_filename, row):\n",
    "    \n",
    "    with open(metadata_filename, 'a') as f:\n",
    "        csvwriter = csv.writer(f)\n",
    "        csvwriter.writerow(row)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the twitter API keys and secrets form enviroment variables \n",
    "\n",
    "consumer_key = os.getenv(\"TWITTER_CONSUMER_KEY\")\n",
    "consumer_secret = os.getenv(\"TWITTER_CONSUMER_SECRET\")\n",
    "access_key = os.getenv(\"TWITTER_ACCESS_KEY\")\n",
    "access_secret = os.getenv(\"TWITTER_ACCESS_SECRET\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_tweets(screen_name, state_name):\n",
    "     \n",
    "    #authorize twitter, initialize tweepy\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_key, access_secret)\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "        \n",
    "    # date account was scraped \n",
    "    date_scraped = datetime.date.today()\n",
    "    # format = MonthDay\n",
    "    date_in_words = date_scraped.strftime(\"%B%d\")\n",
    "    \n",
    "    # assign metadata file name \n",
    "#     metadata = '%s_metadata_%s' % (state_name, date_in_words) + '.csv'\n",
    "    \n",
    "    # row = ('handle', 'handle_id','date_scraped','latest_id',\n",
    "   #            'total_tweets', 'tweets_per_week', 'account_created', \n",
    "  #            'account_status')\n",
    "    \n",
    "    alltweets=[]\n",
    "\n",
    "    \n",
    "    # check if the user exists/calls can be made, otherwise log the error \n",
    "    try:\n",
    "        user = api.get_user(screen_name)\n",
    "        \n",
    "        # check if the user is protected \n",
    "        if user.protected == True:\n",
    "            print('protected')\n",
    "            \n",
    "            # save protected in metadata\n",
    "            row = (screen_name, 'NA','NA','NA','NA','NA','NA','protected')\n",
    "#             metadata_writer(metadata, row)  \n",
    "\n",
    "        # if not, make initial request\n",
    "        \n",
    "        else:\n",
    "            print('not protected')\n",
    "            new_tweets = api.user_timeline(screen_name = screen_name, count=200, exclude_replies=False, tweet_mode=\"extended\")    \n",
    "\n",
    "            #check if there is at least 1 tweet \n",
    "            if len(new_tweets)==0:\n",
    "                print('zero tweets')\n",
    "                \n",
    "                row = (screen_name, 'NA','NA','NA','NA','NA','NA','no tweets')\n",
    "#                 metadata_writer(metadata, row)   \n",
    "       \n",
    "            #check if the account is active\n",
    "            else:    \n",
    "                time_delta = datetime.date.today() - new_tweets[0].created_at.date()\n",
    "\n",
    "                if time_delta.days > 365:\n",
    "\n",
    "                    print('account inactive')\n",
    "                    \n",
    "                    row = (screen_name, 'NA', 'NA','NA','NA','NA','NA','inactive')\n",
    "#                     metadata_writer(metadata, row)   \n",
    "        \n",
    "                        \n",
    "                else:\n",
    "                    \n",
    "                    #If account is active, save the most recent tweets\n",
    "                    alltweets.extend(new_tweets)\n",
    "\n",
    "                    #save the id of the oldest tweet less one\n",
    "                    oldest = alltweets[-1].id - 1\n",
    "                    \n",
    "                    #keep grabbing tweets until there are no tweets left to grab\n",
    "                    while len(new_tweets) > 0:\n",
    "                        \n",
    "                        print (\"getting tweets before %s\" % (oldest))\n",
    "                        \n",
    "                        new_tweets = api.user_timeline(screen_name = screen_name, count=200, max_id=oldest, exclude_replies=False,\n",
    "                                                   tweet_mode=\"extended\")\n",
    "\n",
    "                        #save most recent tweets\n",
    "\n",
    "                        alltweets.extend(new_tweets)\n",
    "\n",
    "                        #update the id of the oldest tweet less one\n",
    "                        \n",
    "                        oldest = alltweets[-1].id - 1\n",
    "\n",
    "                        print (\"...%s tweets downloaded so far\" % (len(alltweets)))\n",
    "                        \n",
    "                    # check how many tweets in the last 7 days\n",
    "                    \n",
    "                    # date 8 days ago\n",
    "                    eight_days_ago = datetime.date.today() - datetime.timedelta(days = 8)\n",
    "                    \n",
    "                    i = 0\n",
    "                    for tweet in alltweets:\n",
    "                        if tweet.created_at.date() < eight_days_ago:\n",
    "                            break\n",
    "                        else:\n",
    "                            i+=1\n",
    "                            \n",
    "                    account_id = alltweets[0].user.id\n",
    "                    latest_tweet_id = alltweets[0].id\n",
    "                    total_tweets = len(alltweets)\n",
    "                    last_week_total_tweets = i\n",
    "                    active_since = alltweets[0].user.created_at.date()\n",
    "                \n",
    "                \n",
    "                    # Make directory for each legislator in the list if it doesnt already exist\n",
    "                    if not os.path.exists(screen_name):\n",
    "                        os.mkdir(screen_name)\n",
    "    \n",
    "\n",
    "                    # save all tweets \n",
    "                    # file_name = 'scren_name/screen_name_Tweets_(date_words)_(latest_tweet_id).json'\n",
    "\n",
    "                    file_name =  '%s/' % screen_name + '%s_Tweets_' % screen_name + date_in_words + '_%s' % latest_tweet_id + '.json'                  \n",
    "                    with open(file_name , 'w', encoding='utf8') as file:\n",
    "                        json.dump([tweet._json for tweet in alltweets], file)\n",
    "\n",
    "                    print(\"writen to file\")\n",
    "\n",
    "                    # write metadata\n",
    "                    row = (screen_name, account_id, date_scraped, latest_tweet_id, total_tweets, last_week_total_tweets, active_since, 'active')\n",
    "#                     metadata_writer(metadata, row) \n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = e.args[0][0]['message']\n",
    "        print(error_msg)\n",
    "\n",
    "        # save error_msg in metadata\n",
    "        row = (screen_name, 'NA','NA','NA','NA','NA','NA', error_msg)\n",
    "#         metadata_writer(metadata, row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df():\n",
    "    data = {'state': ['Ohio','Ohio','Ohio','Nevada','Nevada'],\n",
    "           'year': [2000,2001,2002,2001,2002],\n",
    "           'pop': [1.5,1.7,3.6,2.4,2.9]}\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "create_df()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not protected\n",
      "getting tweets before 497597381399949311\n",
      "...63 tweets downloaded so far\n",
      "writen to file\n"
     ]
    }
   ],
   "source": [
    "get_all_tweets('izzigopal', 'New_York')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ishitagopal/Box/Projects/state_covid_policy'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#         # if the call goes through, check if the user is protected \n",
    "#         if user.protected == True:\n",
    "#             print('protected')\n",
    "#             # save protected in metadata\n",
    "#             row = (screen_name, 'NA','NA','NA','NA','NA','NA','protected')\n",
    "#             metadata_writer(metadata, row)  \n",
    "            \n",
    "#         # if not, make initial request     \n",
    "#         else:\n",
    "#             print('not protected')\n",
    "#             new_tweets = api.user_timeline(screen_name = screen_name, count=200, exclude_replies=False, tweet_mode=\"extended\")    \n",
    "\n",
    "#             #check if there is at least 1 tweet \n",
    "#             if len(new_tweets)==0:\n",
    "#                 print('zero tweets')\n",
    "                \n",
    "#                 row = (screen_name, 'NA','NA','NA','NA','NA','NA','no tweets')\n",
    "#                 metadata_writer(metadata, row)   \n",
    "       \n",
    "\n",
    "#             #check if the account is active\n",
    "#             else:    \n",
    "#                 time_delta = datetime.date.today() - new_tweets[0].created_at.date()\n",
    "\n",
    "#                 if time_delta.days > 365:\n",
    "\n",
    "#                     print('account inactive')\n",
    "                    \n",
    "#                     row = (screen_name, 'NA', 'NA','NA','NA','NA','NA','inactive')\n",
    "#                     metadata_writer(metadata, row)   \n",
    "        \n",
    "                        \n",
    "#                 else:\n",
    "                    \n",
    "#                     #If account is active, save the most recent tweets\n",
    "#                     alltweets.extend(new_tweets)\n",
    "\n",
    "#                     #save the id of the oldest tweet less one\n",
    "#                     oldest = alltweets[-1].id - 1\n",
    "                    \n",
    "#                     #keep grabbing tweets until there are no tweets left to grab\n",
    "#                     while len(new_tweets) > 0:\n",
    "                        \n",
    "#                         print (\"getting tweets before %s\" % (oldest))\n",
    "                        \n",
    "#                         new_tweets = api.user_timeline(screen_name = screen_name, count=200, max_id=oldest, exclude_replies=False,\n",
    "#                                                    tweet_mode=\"extended\")\n",
    "\n",
    "#                         #save most recent tweets\n",
    "\n",
    "#                         alltweets.extend(new_tweets)\n",
    "\n",
    "#                         #update the id of the oldest tweet less one\n",
    "                        \n",
    "#                         oldest = alltweets[-1].id - 1\n",
    "\n",
    "#                         print (\"...%s tweets downloaded so far\" % (len(alltweets)))\n",
    "                        \n",
    "#                     # check how many tweets in the last 7 days\n",
    "                    \n",
    "#                     # date 8 days ago\n",
    "#                     eight_days_ago = datetime.date.today() - datetime.timedelta(days = 8)\n",
    "                    \n",
    "#                     i = 0\n",
    "#                     for tweet in alltweets:\n",
    "#                         if tweet.created_at.date() < eight_days_ago:\n",
    "#                             break\n",
    "#                         else:\n",
    "#                             i+=1\n",
    "                            \n",
    "#                     account_id = alltweets[0].user.id\n",
    "#                     latest_tweet_id = alltweets[0].id\n",
    "#                     total_tweets = len(alltweets)\n",
    "#                     last_week_total_tweets = i\n",
    "#                     active_since = alltweets[0].user.created_at.date()\n",
    "                \n",
    "                \n",
    "#                     # Make directory for each legislator in the list if it doesnt already exist\n",
    "#                     if not os.path.exists(screen_name):\n",
    "#                         os.mkdir(screen_name)\n",
    "    \n",
    "\n",
    "#                     # save all tweets \n",
    "#                     # file_name = 'scren_name/screen_name_Tweets_(date_words)_(latest_tweet_id).json'\n",
    "\n",
    "#                     file_name =  '%s/' % screen_name + '%s_Tweets_' % screen_name + date_in_words + '_%s' % latest_tweet_id + '.json'                  \n",
    "#                     with open(file_name , 'w', encoding='utf8') as file:\n",
    "#                         json.dump([tweet._json for tweet in alltweets], file)\n",
    "\n",
    "#                     print(\"writen to file\")\n",
    "\n",
    "#                     # write metadata\n",
    "#                     row = (screen_name, account_id, date_scraped, latest_tweet_id, total_tweets, last_week_total_tweets, active_since, 'active')\n",
    "#                     metadata_writer(metadata, row)   \n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ishitagopal/Desktop'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_friend_ids(screen_name):\n",
    "    \n",
    "\n",
    "    #authorize twitter, initialize tweepy\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_key, access_secret)\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    try:\n",
    "        friendsids =[]\n",
    "        \n",
    "        for user in tweepy.Cursor(api.friends_ids, screen_name= screen_name,count=5000).items():\n",
    "            friendsids.append(user)\n",
    "          \n",
    "        date_scraped = datetime.date.today().strftime('%B%d')\n",
    "        friends_len = len(friendsids)\n",
    "    \n",
    "        file_name = '%s/%s_FriendsIds_%s_%s.csv' %(screen_name, screen_name, date_scraped, friends_len)\n",
    "        pd.Series(friendsids).to_csv(file_name, index = False, header = False)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        time.sleep(5)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "os.chdir('/Users/ishitagopal/Box/Projects/state_covid_policy/Data/North_Carolina')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'active_accounts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3b84387ceb9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Accounts for which to retry collection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mretry_friends\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mactive_accounts\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscarped_friends\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'active_accounts' is not defined"
     ]
    }
   ],
   "source": [
    "# for account in active_accounts:\n",
    "#     print(account)\n",
    "#     get_friend_ids(account)\n",
    "# Accounts for which friend Ids were scraped successfully \n",
    "\n",
    "# Accounts for which friend Ids were scraped successfully \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def collect_friends(accounts_list):\n",
    "        \n",
    "        filepath_collected_friends = glob.glob('*/*FriendsIds*.csv')\n",
    "        \n",
    "        if len(filepath_collected_friends) == 0:\n",
    "            \n",
    "            for account in account_list:\n",
    "                print(account)\n",
    "                get_friend_ids(account)\n",
    "                \n",
    "        else:\n",
    "        \n",
    "        \n",
    "        completed_friend_collection = [name.split('/')[0] for name in filepath_friends]\n",
    "\n",
    "\n",
    "# Accounts for which to retry collection \n",
    "\n",
    "retry_friends = [x for x in active_accounts if x not in scarped_friends]   \n",
    "\n",
    "\n",
    "while len(retry_friends) > 0:\n",
    "    print(len(retry_friends))\n",
    "    for account in retry_friends:\n",
    "        print(account)\n",
    "        get_friend_ids(account)\n",
    "        \n",
    "    filepath_friends = glob.glob('*/*FriendsIds*.csv')\n",
    "    scarped_friends = [name.split('/')[0] for name in filepath_friends]        \n",
    "    retry_friends = [x for x in active_accounts if x not in scarped_friends]      \n",
    "\n",
    "\n",
    "    \n",
    "#Find duplicates     \n",
    "# from collections import Counter\n",
    "\n",
    "# [k for k,v in Counter(scarped_friends).items() if v>1]\n",
    "\n",
    "print(len(retry_friends))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def continue_timeline_collection(screen_name, last_tweet_id, update_number):\n",
    "    \n",
    "    alltweets = []\n",
    "    \n",
    "    for tweet in tweepy.Cursor(api.user_timeline, screen_name = screen_name, \n",
    "                               since_id = last_tweet_id).items(200):\n",
    "        alltweets.append(tweet)\n",
    "        \n",
    "    # date the  account was scraped \n",
    "    date_scraped = datetime.date.today()   \n",
    "    date_in_words = date_scraped.strftime(\"%B%d\")      # format = MonthDay\n",
    "    \n",
    "    # assign metadata file name \n",
    "    metadata = '%s_metadata_Round%s_%s' % (state_name, update_number, date_in_words) + '.csv'    \n",
    "\n",
    "    \n",
    "    row = (screen_name, account_id, date_scraped, latest_tweet_id, total_tweets, last_week_total_tweets, active_since, 'active')\n",
    "    metadata_writer(metadata, row)\n",
    "    \n",
    "    # Dump tweets to file\n",
    "    # File path = 'screen_name/screenname_Tweets_date_tweetid.json'\n",
    "    \n",
    "    if len(alltweets) > 0:\n",
    "\n",
    "        file_name =  '%s/' % screen_name + '%s_Round2_Tweets_' % screen_name + date_in_words + '_%s' % latest_tweet_id + '.json'                  \n",
    "        with open(file_name , 'w', encoding='utf8') as file:\n",
    "            json.dump([tweet._json for tweet in alltweets], file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_tweets('izzigopal', 'New_Delhi')\n",
    "os.listdir()\n",
    "metadata = glob.glob('*_metadata_*.csv')[0]\n",
    "pd.read_csv(metadata, header = None)\n",
    "continue_timeline_collection()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## How to open saved json tweets\n",
    "\n",
    "#json_data = 'Adam_Morfeld/Adam_Morfeld_Tweets_June24_1275617128461164551.json'\n",
    "#with open(json_data, 'r') as f:\n",
    "#    distros_dict = json.load(f)\n",
    "#distros_dict[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
