{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import datetime\n",
    "import tweepy\n",
    "import json \n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/ishitagopal/Box/Projects/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for writing to metadata \n",
    "# input the metadatfile to write to; input the row to write \n",
    "\n",
    "def metadata_writer(metadata_filename, row):\n",
    "    \n",
    "    with open(metadata_filename, 'a') as f:\n",
    "        csvwriter = csv.writer(f)\n",
    "        csvwriter.writerow(row)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the twitter API keys and secrets form enviroment variables \n",
    "\n",
    "consumer_key = os.getenv(\"TWITTER_CONSUMER_KEY\")\n",
    "consumer_secret = os.getenv(\"TWITTER_CONSUMER_SECRET\")\n",
    "access_key = os.getenv(\"TWITTER_ACCESS_KEY\")\n",
    "access_secret = os.getenv(\"TWITTER_ACCESS_SECRET\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timeline(screen_name, state_name):\n",
    "    \n",
    "        # Authorize twitter, initialize tweepy\n",
    "        auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "        auth.set_access_token(access_key, access_secret)\n",
    "        api = tweepy.API(auth, wait_on_rate_limit=True, \n",
    "                     wait_on_rate_limit_notify=True)\n",
    "\n",
    "    # Assign the name of fields to stored as metadata    \n",
    "    Data = namedtuple(\"Data\", [\"handle\", \"handle_id\", \"date_scraped\", \n",
    "                               \"latest_tweet_id\", \"total_tweets\", \n",
    "                               \"tweets_per_week\", \"account_created_date\", \n",
    "                               \"account_status\"])\n",
    "    \n",
    "    alltweets = []  # Will store all the scraped tweets \n",
    "    \n",
    "    try:\n",
    "        # Make initial call\n",
    "        user = api.get_user(screen_name)\n",
    "        \n",
    "        # Check if the account is protected\n",
    "        if user.protected == True:  \n",
    "            print('protected')\n",
    "            metadata = Data(screen_name, 'NA', 'NA', 'NA', 'NA', 'NA', 'NA',\n",
    "                            'protected')  \n",
    "\n",
    "        # If not, make initial request\n",
    "        else:\n",
    "            print('not protected')\n",
    "            new_tweets = api.user_timeline(screen_name=screen_name, \n",
    "                                           count=200, \n",
    "                                           exclude_replies=False, \n",
    "                                           tweet_mode=\"extended\")    \n",
    "\n",
    "            # Check if the account posted any tweet \n",
    "            if len(new_tweets)==0:\n",
    "                print('zero tweets')\n",
    "                metadata = Data(screen_name, 'NA', 'NA', 'NA', 'NA', 'NA', 'NA',\n",
    "                                'no tweets')\n",
    "       \n",
    "            # Check if the account has been active in the last year\n",
    "            else:    \n",
    "                time_delta = datetime.date.today() - new_tweets[0].created_at.date()\n",
    "                if time_delta.days > 365:\n",
    "                    print('account inactive')\n",
    "                    metadata = Data(screen_name, 'NA', 'NA', 'NA', 'NA', 'NA',\n",
    "                                    'NA', 'inactive')               \n",
    "                else:\n",
    "                    # If the account is active add tweets to alltweets \n",
    "                    alltweets.extend(new_tweets)\n",
    "                    \n",
    "                    # Save the id of the oldest tweet less one\n",
    "                    oldest = alltweets[-1].id - 1\n",
    "                    \n",
    "                    # Keep grabbing tweets until there are no tweets left to grab\n",
    "                    while len(new_tweets) > 0:\n",
    "                        print (\"getting tweets before %s\" % (oldest))\n",
    "                        \n",
    "                        new_tweets = api.user_timeline(\n",
    "                            screen_name = screen_name, \n",
    "                            count=200, max_id=oldest, \n",
    "                            exclude_replies=False,                       \n",
    "                            tweet_mode=\"extended\")\n",
    "\n",
    "                        alltweets.extend(new_tweets)\n",
    "\n",
    "                        # Update the id of the oldest tweet less one\n",
    "                        oldest = alltweets[-1].id - 1\n",
    "                        print (\"...%s tweets downloaded so far\" % \n",
    "                               (len(alltweets)))\n",
    "                        \n",
    "                    # Check how many tweets were posted in the last 7 days\n",
    "                    eight_days_ago = datetime.date.today() - datetime.timedelta(days = 8)\n",
    "                    counter = 0\n",
    "                    for tweet in alltweets:\n",
    "                        if tweet.created_at.date() < eight_days_ago:\n",
    "                            break\n",
    "                        else:\n",
    "                            counter+=1\n",
    "                            \n",
    "                    # Collect fields for the metedata\n",
    "                    date_scraped = datetime.date.today()          \n",
    "                    account_id = alltweets[0].user.id\n",
    "                    latest_tweet_id = alltweets[0].id\n",
    "                    total_tweets = len(alltweets)\n",
    "                    last_week_total_tweets = counter\n",
    "                    active_since = alltweets[0].user.created_at.date()\n",
    "                    \n",
    "                    metadata = Data(screen_name, account_id, date_scraped, \n",
    "                                    latest_tweet_id, total_tweets, \n",
    "                                    last_week_total_tweets, active_since, \n",
    "                                    'active')\n",
    "                    \n",
    "    # Except exception if the user does not exist or is suspended \n",
    "    except tweepy.error.TweepError as e:\n",
    "        if (e.api_code == 50) or (e.api_code == 63):\n",
    "            print(e)\n",
    "            metadata = Data(screen_name, 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', \n",
    "                            e.args[0][0]['message'])\n",
    "\n",
    "    # Return results         \n",
    "    if len(alltweets)>0:\n",
    "        results_dic = {'tweets': alltweets, 'metadata': metadata}\n",
    "    else:\n",
    "        results_dic = {'tweets':'NA', 'metadata' : metadata}\n",
    "        \n",
    "    return results_dic               \n",
    "                               \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not protected\n",
      "getting tweets before 497597381399949311\n",
      "...62 tweets downloaded so far\n"
     ]
    }
   ],
   "source": [
    "results = get_timeline('izzigopal', 'Ten')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(handle='izzigopal', handle_id=2502041252, date_scraped=datetime.date(2020, 8, 6), latest_tweet_id=1283661986656657411, total_tweets=62, tweets_per_week=0, account_created_date=datetime.date(2014, 5, 17), account_status='active')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['metadata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('izzigopal',\n",
       " 2502041252,\n",
       " datetime.date(2020, 8, 6),\n",
       " 1283661986656657411,\n",
       " 62,\n",
       " 0,\n",
       " datetime.date(2014, 5, 17),\n",
       " 'active')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['metadata'][0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "handles = ['izzigopal', 'ganeshgorti']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a metadat file with fixed number of entries \n",
    "# The index \n",
    "metadata = df = pd.DataFrame(columns=['handle_id','date_scraped',\n",
    "                                      'latest_tweet_id','total_tweets',\n",
    "                                      'tweets_per_week','account_created_date',\n",
    "                                     'account_status'],\n",
    "                            index=handles)\n",
    "\n",
    "today = datetime.date.today().strftime(\"%B%d\") \n",
    "fname = '%s_metadata_%s' % (state_name, today) + '.csv'\n",
    "\n",
    "metadata.to_csv(fname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.loc['izzigopal'] = results['metadata'][1:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>handle_id</th>\n",
       "      <th>date_scraped</th>\n",
       "      <th>latest_tweet_id</th>\n",
       "      <th>total_tweets</th>\n",
       "      <th>tweets_per_week</th>\n",
       "      <th>account_created_date</th>\n",
       "      <th>account_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>izzigopal</td>\n",
       "      <td>2502041252</td>\n",
       "      <td>2020-08-06</td>\n",
       "      <td>1283661986656657411</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-05-17</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ganeshgorti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              handle_id date_scraped      latest_tweet_id total_tweets  \\\n",
       "izzigopal    2502041252   2020-08-06  1283661986656657411           62   \n",
       "ganeshgorti         NaN          NaN                  NaN          NaN   \n",
       "\n",
       "            tweets_per_week account_created_date account_status  \n",
       "izzigopal                 0           2014-05-17         active  \n",
       "ganeshgorti             NaN                  NaN            NaN  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>handle</th>\n",
       "      <th>handle_id</th>\n",
       "      <th>date_scraped</th>\n",
       "      <th>latest_tweet_id</th>\n",
       "      <th>total_tweets</th>\n",
       "      <th>tweets_per_week</th>\n",
       "      <th>account_created_date</th>\n",
       "      <th>account_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>izzigopal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          handle handle_id date_scraped latest_tweet_id total_tweets  \\\n",
       "izzigopal    NaN       NaN          NaN             NaN          NaN   \n",
       "\n",
       "          tweets_per_week account_created_date account_status  \n",
       "izzigopal             NaN                  NaN            NaN  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata.loc['izzigopal'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'' does not exist: b''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-f948ed747485>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'' does not exist: b''"
     ]
    }
   ],
   "source": [
    "# # Download tweets and write to a text file \n",
    "# # Assumes you are in the state folder \n",
    "\n",
    "# # Open metadata file \n",
    "\n",
    "\n",
    "# metadata = pd.read_csv('')\n",
    "\n",
    "# for handle in handles:\n",
    "\n",
    "#     results = get_timeline(handle, state_name)\n",
    "\n",
    "#     # Edit/Add to metadat\n",
    "afile \n",
    "    \n",
    "#     metadata.loc[handle] = results['metadata'][1:8]\n",
    "    \n",
    "#     if results['tweets'] != 'NA':\n",
    "        \n",
    "#         # Make a directory for the account if it doesnt already exist\n",
    "#         # Assumes you are in the state directory \n",
    "#         if not os.path.exists(handle):\n",
    "#             os.mkdir(handle)\n",
    "            \n",
    "#             # Dump tweets to file\n",
    "#             file_name =  '%s/' % handle + '%s_Tweets_' % handle + '.json'\n",
    "#             print(file_name)\n",
    "#             with open(file_name , 'w', encoding='utf8') as file:\n",
    "#                 json.dump([tweet._json for tweet in results['tweets']], file)\n",
    "#                 print(\"writen to file\")\n",
    "                \n",
    "        \n",
    "# metadata.to_csv('')   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not protected\n",
      "getting tweets before 497597381399949311\n",
      "...63 tweets downloaded so far\n",
      "writen to file\n"
     ]
    }
   ],
   "source": [
    "# get_all_tweets('izzigopal', 'New_York')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#         # if the call goes through, check if the user is protected \n",
    "#         if user.protected == True:\n",
    "#             print('protected')\n",
    "#             # save protected in metadata\n",
    "#             row = (screen_name, 'NA','NA','NA','NA','NA','NA','protected')\n",
    "#             metadata_writer(metadata, row)  \n",
    "            \n",
    "#         # if not, make initial request     \n",
    "#         else:\n",
    "#             print('not protected')\n",
    "#             new_tweets = api.user_timeline(screen_name = screen_name, count=200, exclude_replies=False, tweet_mode=\"extended\")    \n",
    "\n",
    "#             #check if there is at least 1 tweet \n",
    "#             if len(new_tweets)==0:\n",
    "#                 print('zero tweets')\n",
    "                \n",
    "#                 row = (screen_name, 'NA','NA','NA','NA','NA','NA','no tweets')\n",
    "#                 metadata_writer(metadata, row)   \n",
    "       \n",
    "\n",
    "#             #check if the account is active\n",
    "#             else:    \n",
    "#                 time_delta = datetime.date.today() - new_tweets[0].created_at.date()\n",
    "\n",
    "#                 if time_delta.days > 365:\n",
    "\n",
    "#                     print('account inactive')\n",
    "                    \n",
    "#                     row = (screen_name, 'NA', 'NA','NA','NA','NA','NA','inactive')\n",
    "#                     metadata_writer(metadata, row)   \n",
    "        \n",
    "                        \n",
    "#                 else:\n",
    "                    \n",
    "#                     #If account is active, save the most recent tweets\n",
    "#                     alltweets.extend(new_tweets)\n",
    "\n",
    "#                     #save the id of the oldest tweet less one\n",
    "#                     oldest = alltweets[-1].id - 1\n",
    "                    \n",
    "#                     #keep grabbing tweets until there are no tweets left to grab\n",
    "#                     while len(new_tweets) > 0:\n",
    "                        \n",
    "#                         print (\"getting tweets before %s\" % (oldest))\n",
    "                        \n",
    "#                         new_tweets = api.user_timeline(screen_name = screen_name, count=200, max_id=oldest, exclude_replies=False,\n",
    "#                                                    tweet_mode=\"extended\")\n",
    "\n",
    "#                         #save most recent tweets\n",
    "\n",
    "#                         alltweets.extend(new_tweets)\n",
    "\n",
    "#                         #update the id of the oldest tweet less one\n",
    "                        \n",
    "#                         oldest = alltweets[-1].id - 1\n",
    "\n",
    "#                         print (\"...%s tweets downloaded so far\" % (len(alltweets)))\n",
    "                        \n",
    "#                     # check how many tweets in the last 7 days\n",
    "                    \n",
    "#                     # date 8 days ago\n",
    "#                     eight_days_ago = datetime.date.today() - datetime.timedelta(days = 8)\n",
    "                    \n",
    "#                     i = 0\n",
    "#                     for tweet in alltweets:\n",
    "#                         if tweet.created_at.date() < eight_days_ago:\n",
    "#                             break\n",
    "#                         else:\n",
    "#                             i+=1\n",
    "                            \n",
    "#                     account_id = alltweets[0].user.id\n",
    "#                     latest_tweet_id = alltweets[0].id\n",
    "#                     total_tweets = len(alltweets)\n",
    "#                     last_week_total_tweets = i\n",
    "#                     active_since = alltweets[0].user.created_at.date()\n",
    "                \n",
    "                \n",
    "#                     # Make directory for each legislator in the list if it doesnt already exist\n",
    "#                     if not os.path.exists(screen_name):\n",
    "#                         os.mkdir(screen_name)\n",
    "    \n",
    "\n",
    "#                     # save all tweets \n",
    "#                     # file_name = 'scren_name/screen_name_Tweets_(date_words)_(latest_tweet_id).json'\n",
    "\n",
    "#                     file_name =  '%s/' % screen_name + '%s_Tweets_' % screen_name + date_in_words + '_%s' % latest_tweet_id + '.json'                  \n",
    "#                     with open(file_name , 'w', encoding='utf8') as file:\n",
    "#                         json.dump([tweet._json for tweet in alltweets], file)\n",
    "\n",
    "#                     print(\"writen to file\")\n",
    "\n",
    "#                     # write metadata\n",
    "#                     row = (screen_name, account_id, date_scraped, latest_tweet_id, total_tweets, last_week_total_tweets, active_since, 'active')\n",
    "#                     metadata_writer(metadata, row)   \n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ishitagopal/Desktop'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_friend_ids(screen_name):\n",
    "    \n",
    "\n",
    "    #authorize twitter, initialize tweepy\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_key, access_secret)\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    try:\n",
    "        friendsids =[]\n",
    "        \n",
    "        for user in tweepy.Cursor(api.friends_ids, screen_name= screen_name,count=5000).items():\n",
    "            friendsids.append(user)\n",
    "          \n",
    "        date_scraped = datetime.date.today().strftime('%B%d')\n",
    "        friends_len = len(friendsids)\n",
    "    \n",
    "        file_name = '%s/%s_FriendsIds_%s_%s.csv' %(screen_name, screen_name, date_scraped, friends_len)\n",
    "        pd.Series(friendsids).to_csv(file_name, index = False, header = False)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        time.sleep(5)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "os.chdir('/Users/ishitagopal/Box/Projects/state_covid_policy/Data/North_Carolina')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'active_accounts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3b84387ceb9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Accounts for which to retry collection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mretry_friends\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mactive_accounts\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscarped_friends\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'active_accounts' is not defined"
     ]
    }
   ],
   "source": [
    "# for account in active_accounts:\n",
    "#     print(account)\n",
    "#     get_friend_ids(account)\n",
    "# Accounts for which friend Ids were scraped successfully \n",
    "\n",
    "# Accounts for which friend Ids were scraped successfully \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def collect_friends(accounts_list):\n",
    "        \n",
    "        filepath_collected_friends = glob.glob('*/*FriendsIds*.csv')\n",
    "        \n",
    "        if len(filepath_collected_friends) == 0:\n",
    "            \n",
    "            for account in account_list:\n",
    "                print(account)\n",
    "                get_friend_ids(account)\n",
    "                \n",
    "        else:\n",
    "        \n",
    "        \n",
    "        completed_friend_collection = [name.split('/')[0] for name in filepath_friends]\n",
    "\n",
    "\n",
    "# Accounts for which to retry collection \n",
    "\n",
    "retry_friends = [x for x in active_accounts if x not in scarped_friends]   \n",
    "\n",
    "\n",
    "while len(retry_friends) > 0:\n",
    "    print(len(retry_friends))\n",
    "    for account in retry_friends:\n",
    "        print(account)\n",
    "        get_friend_ids(account)\n",
    "        \n",
    "    filepath_friends = glob.glob('*/*FriendsIds*.csv')\n",
    "    scarped_friends = [name.split('/')[0] for name in filepath_friends]        \n",
    "    retry_friends = [x for x in active_accounts if x not in scarped_friends]      \n",
    "\n",
    "\n",
    "    \n",
    "#Find duplicates     \n",
    "# from collections import Counter\n",
    "\n",
    "# [k for k,v in Counter(scarped_friends).items() if v>1]\n",
    "\n",
    "print(len(retry_friends))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def continue_timeline_collection(screen_name, last_tweet_id, update_number):\n",
    "    \n",
    "    alltweets = []\n",
    "    \n",
    "    for tweet in tweepy.Cursor(api.user_timeline, screen_name = screen_name, \n",
    "                               since_id = last_tweet_id).items(200):\n",
    "        alltweets.append(tweet)\n",
    "        \n",
    "    # date the  account was scraped \n",
    "    date_scraped = datetime.date.today()   \n",
    "    date_in_words = date_scraped.strftime(\"%B%d\")      # format = MonthDay\n",
    "    \n",
    "    # assign metadata file name \n",
    "    metadata = '%s_metadata_Round%s_%s' % (state_name, update_number, date_in_words) + '.csv'    \n",
    "\n",
    "    \n",
    "    row = (screen_name, account_id, date_scraped, latest_tweet_id, total_tweets, last_week_total_tweets, active_since, 'active')\n",
    "    metadata_writer(metadata, row)\n",
    "    \n",
    "    # Dump tweets to file\n",
    "    # File path = 'screen_name/screenname_Tweets_date_tweetid.json'\n",
    "    \n",
    "    if len(alltweets) > 0:\n",
    "\n",
    "        file_name =  '%s/' % screen_name + '%s_Round2_Tweets_' % screen_name + date_in_words + '_%s' % latest_tweet_id + '.json'                  \n",
    "        with open(file_name , 'w', encoding='utf8') as file:\n",
    "            json.dump([tweet._json for tweet in alltweets], file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_tweets('izzigopal', 'New_Delhi')\n",
    "os.listdir()\n",
    "metadata = glob.glob('*_metadata_*.csv')[0]\n",
    "pd.read_csv(metadata, header = None)\n",
    "continue_timeline_collection()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## How to open saved json tweets\n",
    "\n",
    "#json_data = 'Adam_Morfeld/Adam_Morfeld_Tweets_June24_1275617128461164551.json'\n",
    "#with open(json_data, 'r') as f:\n",
    "#    distros_dict = json.load(f)\n",
    "#distros_dict[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
